## ICCV2023 paper list

ICCV2023结果陆续都出来了，收到了很多朋友中稿的消息，ICCV 2023今年一共收录 2100多篇，自动驾驶之心也第一时间进行了跟进，将已确定中稿的工作分享给大家，后面将会持续更新！

后面将会按照3D目标检测、BEV、协同感知、语义分割、点云、SLAM、大模型、NeRF、端到端、多模态融合等多个方向罗列！

**如果您要投稿，欢迎提交Issue,或者联系邮箱autodrivingtech@163.com，我们会及时收录！**



本内容由自[公众号【自动驾驶之心】](https://mp.weixin.qq.com/s?__biz=Mzg2NzUxNTU1OA==&mid=2247542481&idx=1&sn=c6d8609491a128233c3c3b91d68d22a6&chksm=ceb80b18f9cf820e789efd75947633aec9d2f1e8b58c29e5051c05a64b21ae63c244d54886a1&token=11182364&lang=zh_CN#rd) 团队整理，自动驾驶之心建立了一系列技术交流群，面向自动驾驶与AI领域，包括：**目标检测、语义分割、全景分割、实例分割、车道线、目标跟踪、3D目标检测、多模态感知、BEV感知、Occupancy、多传感器融合、多传感器标定、transformer、大模型、点云处理、端到端自动驾驶、SLAM、光流估计、深度估计、轨迹预测、高精地图、NeRF、规划控制、模型部署落地、自动驾驶仿真测试、产品经理、硬件配置、AI求职交流等**！

如果您有需要，欢迎加入自动驾驶之心：[技术交流群](https://mp.weixin.qq.com/s/G0zMQgCHNbeXKOei3Mgtwg)



## 1）OCC感知

**SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving**

- Paper：[SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving](https://arxiv.org/abs/2303.09551)
- Code：https://github.com/weiyithu/SurroundOcc

**OccNet: Scene as Occupancy**

- Paper：[Scene as Occupancy](https://arxiv.org/pdf/2306.02851.pdf)
- Code：https://github.com/OpenDriveLab/OccNet

**OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction**

- Paper: https://arxiv.org/pdf/2304.05316.pdf
- Code: https://github.com/zhangyp15/OccFormer

**OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception**

- Paper: https://arxiv.org/pdf/2303.03991.pdf
- Code: https://github.com/JeffWang987/OpenOccupancy

## 2)  端到端自动驾驶

**VAD: Vectorized Scene Representation for Efficient Autonomous Driving**

- Paper: https://arxiv.org/pdf/2303.12077.pdf
- Code: https://github.com/hustvl/VAD

## 3）协同感知

**Among Us: Adversarially Robust Collaborative Perception by Consensus**

-  Paper: https://arxiv.org/pdf/2303.09495.pdf
- Code: https://github.com/coperception/ROBOSAC

**HM-ViT: Hetero-modal Vehicle-to-Vehicle Cooperative perception with vision transformer**

- Paper: https://arxiv.org/pdf/2304.10628.pdf

**Optimizing the Placement of Roadside LiDARs for Autonomous Driving**

待更新！

**UMC: A Unified Bandwidth-efficient and Multi-resolution based Collaborative Perception Framework**

- Paper: https://arxiv.org/pdf/2303.12400.pdf

## 4）3D目标检测

**PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images**

- Paper: https://arxiv.org/abs/2206.01256
- Code: https://github.com/megvii-research/PETR

**StreamPETR: Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection**

- Paper: https://arxiv.org/pdf/2303.11926.pdf
- Code: https://github.com/exiawsh/StreamPETR.git

**Cross Modal Transformer: Towards Fast and Robust 3D Object Detection**

- Paper: https://arxiv.org/pdf/2301.01283.pdf
- Code: https://github.com/junjie18/CMT.git

**DQS3D: Densely-matched Quantization-aware Semi-supervised 3D Detection**

- Paper: https://arxiv.org/abs/2304.13031
- Code: https://github.com/AIR-DISCOVER/DQS3D

**SparseFusion: Fusing Multi-Modal Sparse Representations for Multi-Sensor 3D Object Detection**

- Paper: https://arxiv.org/abs/2304.14340
- Code: https://github.com/yichen928/SparseFusion

**MetaBEV: Solving Sensor Failures for BEV Detection and Map Segmentation**

-  Paper: https://arxiv.org/pdf/2304.09801.pdf
-  Code: https://github.com/ChongjianGE/MetaBEV

**Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction**

- Paper: https://arxiv.org/pdf/2304.00967.pdf
- Code: https://github.com/Sense-X/HoP

## 5）语义分割

**Rethinking Range View Representation for LiDAR Segmentation**

Paper：https://arxiv.org/pdf/2303.05367.pdf

**UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the OpenPCSeg Codebase**

已收录，arxiv上暂未放出！

**Segment Anything**

- Paper: https://arxiv.org/abs/2304.02643
- Code: https://github.com/facebookresearch/segment-anything

**MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation**

- Paper: https://arxiv.org/abs/2304.09913
- Code: https://github.com/shjo-april/MARS

**Tube-Link: A Flexible Cross Tube Baseline for Universal Video Segmentation**

- Paper: https://arxiv.org/pdf/2303.12782.pdf

- Code: https://github.com/lxtGH/Tube-Link

**Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation**

- Paper: 待更新！
- Code: https://github.com/lizhaoliu-Lec/CPCM

## 6）点云感知

**Robo3D: Towards Robust and Reliable 3D Perception against Corruptions**

- Paper：https://arxiv.org/pdf/2303.17597.pdf
- Code：https://github.com/ldkong1205/Robo3D

**Implicit Autoencoder for Point Cloud Self-supervised Representation Learning**

- Paper: https://arxiv.org/pdf/2201.00785.pdf
- Code: https://github.com/SimingYan/IAE

**P2C: Self-Supervised Point Cloud Completion from Single Partial Clouds**

- Paper: 
- Code: https://github.com/CuiRuikai/Partial2Complete

**CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth Pre-training**

- Paper: https://arxiv.org/pdf/2210.01055.pdf
- Code: https://github.com/tyhuang0428/CLIP2Point

## 7）目标跟踪

**PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework**

- Paper: https://arxiv.org/pdf/2211.11629.pdf
- Code: https://github.com/Jaraxxus-Me/PVT_pp

**Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers**

- Paper: 待更新！
- Code: https://github.com/ZHU-Zhiyu/High-Rank_RGB-Event_Tracker

**ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking**

- Paper: 待更新！
- Code: https://github.com/chengche6230/ReST

## 8)  轨迹预测

**EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting**

- Paper: https://arxiv.org/pdf/2307.09306.pdf
- Code: https://github.com/InhwanBae/EigenTrajectory

## 9）NeRF

**IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis**

- Paper: https://arxiv.org/abs/2210.00647
- Code: https://github.com/zju3dv/IntrinsicNeRF

**SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields**

- Paper: https://arxiv.org/pdf/2212.02501.pdf
- Code: https://github.com/astra-vision/SceneRF

